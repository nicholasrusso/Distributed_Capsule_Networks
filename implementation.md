## Discussing How to Distribute Capsule Networks

Throughout the last week, our team has talked a lot about how we are going to distribute a capsule network implementation.  We decided to use Tensorflow with Keras. As a starting point, we will work on improving Xifeng Guo’s implementation of Hinton’s paper, [found on GitHub](https://github.com/XifengGuo/CapsNet-Keras). Currently, his results are the closest any open source version has come to Hinton’s original results.

To the best of our knowledge, there are two approaches to distributing the training of a capsule network. First, we can train the model by sending a copy of the capsule network to each machine along with a subset of the training data. Each machine will adjust the weights in the network using dynamic routing outlined in the paper by Hinton et al. Upon completion of a training iteration, each machine will send its adjusted capsule network to a central machine which will aggregate the results into a final capsule network. This process can be repeated for several training epochs until a satisfactory accuracy is achieved. Second, we can focus on distributing dynamic routing. In dynamic routing, there is a sequence of steps that requires calculations on each capsule in a layer. These capsule calculations are independent and a bottleneck for performance when run sequentially. By distributing these calculations, we expect to see a significant speedup in the training process.

For a more detailed description of the various types of paralellism including some useful images, see [Nick's post on "Data Parallelism vs Model Parallelism"](parallelism.md).

## Experiment Time!
I have always thought it was a bit silly that the field of computer science was called "computer science". As far as I had ever experienced, there was nothing really science-y about it. You take technologies that have already been proven to be useful, and use them to make something. That was pretty much the extent of my experience. Recently, however, I finally got the opportunity to learn more about where the word "science" really comes from. This quarter I got the opportunity to play around with and run experiments on capsule networks.

Setting up my environment was sort of a pain in the butt. I had to simultaneously learn how to do several things in order to get started. First, there was relearning how python works. To be honest, I haven't used this language much since I first started learning how to program. Second, I had to learn how to interface with TensorFlow. Thankfully, Keras helped abstract a lot of the ugly math away from me, so this part wasn't as difficult. Next, I had to somehow figure out how to incorporate Horovod into all of this so that I could distribute my network. Thankfully this was surprisingly easy. I had to copy a few lines of code that they recommended here and there throughout my python script and then I was good to go on that end. Finally, after hours of playing around with different versions of libraries and setting different environment variables, I was running experiments to train capsule networks to identify hand-written digits provided by the MNIST dataset. 

At the start of each experiment, my console spit out something like this:

![alt text](FreeMemory.png)

In this experiment I am running on five machines. This image details the specifications and resources available on the five machines I was using. These machines are actually open to use by anyone at my school. The highest I have ever seen the number highlighted in the image is around 4GB. I needed several GB of free memory if I wanted to run my experiments at any reasonable speed. When I started trying to run experiments, I actually kept getting a bunch of memory errors. I then checked the amount of free memory and realized that on one of the machines I only had around 40MB. Somebody was running a program on one of the machines I was attempting to use that was eating up all of the memory, leaving me with basically nothing to work with. It was at this moment that I realized how much of a competition it would be to get access to several computers simultaneously, each with enough free memory to train my network.

Finally, I was able to get a few GB on 5 machines and successfully ran some experiments. In order to understand any of the data that was being recorded, I used a tool called "TensorBoard" which took in some of the log files generated throughout the training and displayed their data in nice graphs. Here is an example of what TensorBoard provided me with after a couple of experiments.

![alt text](MichaelTensorBoard.png)

TensorBoard allowed me to see multiple experiment results side-by-side. The two experiments shown in the graphs above were identical besides differing in batch size. Changing the batch size affects the number of images that each machine in my distributed system receives for training during each epoch. The most important graphs I analyzed were the learning rate (lr) and the validation accuracy (val_capsnet_acc). For both of the graphs the x-axis is the number of epochs. As you can see in the image, I ran each experiment for 100 epochs. On the learning rate graph, you can see the numbers rise up only to suddenly fall after 5 epochs. This is because we used a warm-up, which guarantees a gradual increase in learning rate for the first five epochs before it begins to decay. After the 5th epoch, we decayed the learning rate gradually in order to have a better gradient descent. As far as I have learned, you pretty much always want to decay your learning rate, but the degree to which you decay depends on the problem you are training your network to solve. Finally, we see in the graph of the validation accuracy that the networks that I trained to recognize hand-written digits got to about a 99.6 percent accuracy. Not too bad if I don't say so myself.
